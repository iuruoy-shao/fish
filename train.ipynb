{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import QLearningAgent\n",
    "from verify_data import FishGame, ParseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/1-15_11:15.txt', 'r') as f:\n",
    "    game = FishGame(f.readlines())\n",
    "    game.rotate('PB')\n",
    "    state = game.to_state()\n",
    "with open('test.txt', 'w') as f:\n",
    "    f.writelines([f'{row}\\n' for row in state.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-3_15:27.txt\n",
      "12-10_11:07.txt\n",
      "12-3_14:05.txt\n",
      "12-3_11:30.txt\n",
      "12-3_14:27.txt\n",
      "12-6_11:08.txt\n",
      "12-3_11:12.txt\n",
      "12-4_11:11.txt\n",
      "1-15_11:15.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# TODO: implement shuffling of teammmates & opponents\n",
    "memories = []\n",
    "for filename in os.listdir('data'):\n",
    "    filepath = os.path.join('data', filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        try:\n",
    "            print(f\"{filename}\")\n",
    "            game = FishGame(f.readlines())\n",
    "            for player in game.players:\n",
    "                memories += game.memory(player)\n",
    "        except ParseError as e:\n",
    "            print(f\"{filename}: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent()\n",
    "agent.train_on_data(memories, [], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = agent.unpack_batch([memories[0]])\n",
    "agent.q_network(agent.tensor(batch['state']), agent.action_masks(*batch['mask_dep'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_path = f'models/fish_agent_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pth'\n",
    "agent.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/Fish/.env/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = QLearningAgent(memories) \n",
    "agent.load_model('models/fish_agent_20250407_145324.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/Fish/agent.py:74: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  return torch.BoolTensor(x).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0 finished, 6832 memories collected\n",
      "Game 1 finished, 6832 memories collected\n",
      "epoch 0, train loss 13.80706, test loss None, lr 0.0008\n",
      "Game 2 finished, 6832 memories collected\n",
      "epoch 0, train loss 10.8206, test loss None, lr 0.0008\n",
      "Game 3 finished, 6832 memories collected\n",
      "epoch 0, train loss 13.19224, test loss None, lr 0.0008\n",
      "Game 4 finished, 6832 memories collected\n",
      "epoch 0, train loss 13.5491, test loss None, lr 0.0008\n",
      "Game 5 finished, 6832 memories collected\n",
      "epoch 0, train loss 8.99714, test loss None, lr 0.0008\n",
      "Game 6 finished, 6832 memories collected\n",
      "epoch 0, train loss 11.43613, test loss None, lr 0.0008\n",
      "Game 7 finished, 6832 memories collected\n",
      "epoch 0, train loss 11.70345, test loss None, lr 0.0008\n",
      "Game 8 finished, 6832 memories collected\n",
      "epoch 0, train loss 10.44604, test loss None, lr 0.0008\n",
      "Game 9 finished, 6832 memories collected\n",
      "epoch 0, train loss 8.00655, test loss None, lr 0.0008\n",
      "Game 10 finished, 6832 memories collected\n",
      "epoch 0, train loss 5.59924, test loss None, lr 0.0008\n",
      "Game 11 finished, 6832 memories collected\n",
      "epoch 0, train loss 4.78769, test loss None, lr 0.0008\n",
      "Game 12 finished, 6832 memories collected\n",
      "epoch 0, train loss 3.94566, test loss None, lr 0.0008\n",
      "Game 13 finished, 6832 memories collected\n",
      "epoch 0, train loss 3.01301, test loss None, lr 0.0008\n",
      "Game 14 finished, 6832 memories collected\n",
      "epoch 0, train loss 8.38156, test loss None, lr 0.0008\n",
      "Game 15 finished, 6832 memories collected\n",
      "epoch 0, train loss 12.14267, test loss None, lr 0.0008\n",
      "Game 16 finished, 6832 memories collected\n",
      "epoch 0, train loss 10.56112, test loss None, lr 0.0008\n",
      "Game 17 finished, 6832 memories collected\n",
      "epoch 0, train loss 13.51856, test loss None, lr 0.0008\n",
      "Game 18 finished, 6832 memories collected\n",
      "epoch 0, train loss 10.19671, test loss None, lr 0.0008\n",
      "Game 19 finished, 6832 memories collected\n",
      "epoch 0, train loss 12.27661, test loss None, lr 0.0008\n",
      "Game 20 finished, 6832 memories collected\n",
      "epoch 0, train loss 10.93666, test loss None, lr 0.0008\n",
      "Game 21 finished, 6832 memories collected\n",
      "epoch 0, train loss 9.11353, test loss None, lr 0.0008\n",
      "Game 22 finished, 6832 memories collected\n",
      "epoch 0, train loss 5.17029, test loss None, lr 0.0008\n",
      "Game 23 finished, 6832 memories collected\n",
      "epoch 0, train loss 4.40633, test loss None, lr 0.0008\n",
      "Game 24 finished, 6832 memories collected\n",
      "epoch 0, train loss 5.71471, test loss None, lr 0.0008\n",
      "Game 25 finished, 6832 memories collected\n",
      "epoch 0, train loss 6.6116, test loss None, lr 0.0008\n",
      "Game 26 finished, 6832 memories collected\n",
      "epoch 0, train loss 4.616, test loss None, lr 0.0008\n",
      "Game 27 finished, 6832 memories collected\n",
      "epoch 0, train loss 2.20883, test loss None, lr 0.0008\n",
      "Game 28 finished, 6832 memories collected\n",
      "epoch 0, train loss 1.79894, test loss None, lr 0.0008\n",
      "Game 29 finished, 6832 memories collected\n",
      "epoch 0, train loss 1.9787, test loss None, lr 0.0008\n",
      "Game 30 finished, 6832 memories collected\n",
      "epoch 0, train loss 1.79647, test loss None, lr 0.0008\n",
      "Game 31 finished, 6832 memories collected\n",
      "epoch 0, train loss 0.85244, test loss None, lr 0.0008\n",
      "Game 32 finished, 6832 memories collected\n",
      "epoch 0, train loss 3.14405, test loss None, lr 0.0008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_self_play\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/fish_agent_20250406_213513.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/agent.py:225\u001b[39m, in \u001b[36mQLearningAgent.train_self_play\u001b[39m\u001b[34m(self, n_games, update_rate, epochs, path)\u001b[39m\n\u001b[32m    223\u001b[39m memories_batch = []\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_games):\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     game, memory = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimulate_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m     memories_batch += memory\n\u001b[32m    227\u001b[39m     memories += memory \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(game.datarows) > \u001b[32m50\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/agent.py:271\u001b[39m, in \u001b[36mQLearningAgent.simulate_game\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    269\u001b[39m memories = []\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m player \u001b[38;5;129;01min\u001b[39;00m game.players:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     memories += \u001b[43mgame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m game, memories\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/verify_data.py:211\u001b[39m, in \u001b[36mFishGame.memory\u001b[39m\u001b[34m(self, player)\u001b[39m\n\u001b[32m    196\u001b[39m is_ask = \u001b[38;5;28;01mlambda\u001b[39;00m i: \u001b[38;5;129;01mnot\u001b[39;00m state[i][\u001b[32m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m is_player(i)\n\u001b[32m    197\u001b[39m is_call = \u001b[38;5;28;01mlambda\u001b[39;00m i: state[i][\u001b[32m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m is_player(i)\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [{\n\u001b[32m    199\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.get_state(i, player, state), \u001b[38;5;66;03m# invert sequential order, pad up to 200,\u001b[39;00m\n\u001b[32m    200\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m'\u001b[39m: np.array(\u001b[38;5;28mself\u001b[39m.rewards[i]).reshape(-\u001b[32m1\u001b[39m), \u001b[38;5;66;03m# invert if player on odd team\u001b[39;00m\n\u001b[32m    201\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maction\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m    202\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m: np.array([\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_call(i) \u001b[38;5;28;01melse\u001b[39;00m [\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m]),\n\u001b[32m    203\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcall_set\u001b[39m\u001b[33m'\u001b[39m: state[i][\u001b[32m1\u001b[39m+\u001b[32m8\u001b[39m:\u001b[32m1\u001b[39m+\u001b[32m8\u001b[39m+\u001b[32m9\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_call(i) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    204\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcall_cards\u001b[39m\u001b[33m'\u001b[39m: state[i][\u001b[32m1\u001b[39m+\u001b[32m8\u001b[39m+\u001b[32m9\u001b[39m:\u001b[32m1\u001b[39m+\u001b[32m8\u001b[39m+\u001b[32m9\u001b[39m+\u001b[32m24\u001b[39m].reshape((\u001b[32m6\u001b[39m,\u001b[32m4\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m is_call(i) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    205\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mask_person\u001b[39m\u001b[33m'\u001b[39m: state[i][\u001b[32m9\u001b[39m:\u001b[32m9\u001b[39m+\u001b[32m8\u001b[39m][\u001b[32m1\u001b[39m::\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_ask(i) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[32m    206\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mask_set\u001b[39m\u001b[33m'\u001b[39m: state[i][\u001b[32m9\u001b[39m+\u001b[32m8\u001b[39m:\u001b[32m9\u001b[39m+\u001b[32m8\u001b[39m+\u001b[32m9\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_ask(i) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    207\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mask_card\u001b[39m\u001b[33m'\u001b[39m: state[i][\u001b[32m9\u001b[39m+\u001b[32m8\u001b[39m+\u001b[32m9\u001b[39m:\u001b[32m9\u001b[39m+\u001b[32m8\u001b[39m+\u001b[32m9\u001b[39m+\u001b[32m6\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_ask(i) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    208\u001b[39m     },\n\u001b[32m    209\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnext_state\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.get_state(i+\u001b[32m1\u001b[39m, player, state),\n\u001b[32m    210\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmask_dep\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.mask_dep(i, player),\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnext_mask_dep\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask_dep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m } \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.last_hand_index(player))]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/verify_data.py:224\u001b[39m, in \u001b[36mFishGame.mask_dep\u001b[39m\u001b[34m(self, i, player)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmask_dep\u001b[39m(\u001b[38;5;28mself\u001b[39m, i, player):\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    222\u001b[39m         \u001b[33m'\u001b[39m\u001b[33magent_index\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.players.index(player),\n\u001b[32m    223\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhand\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.encode_hand(\u001b[38;5;28mself\u001b[39m.hands[i][player], flatten=\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;66;03m# 9x6\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m         \u001b[33m'\u001b[39m\u001b[33msets_remaining\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msets_remaining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    225\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcards_remaining\u001b[39m\u001b[33m'\u001b[39m: np.array([\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.hands[i][player]) \u001b[38;5;28;01mfor\u001b[39;00m player \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.players] \n\u001b[32m    226\u001b[39m                                     + ([\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.players) == \u001b[32m6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m [])) \u001b[38;5;66;03m# pad to length 8\u001b[39;00m\n\u001b[32m    227\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/verify_data.py:217\u001b[39m, in \u001b[36mFishGame.sets_remaining\u001b[39m\u001b[34m(self, i)\u001b[39m\n\u001b[32m    215\u001b[39m cards_remaining = np.zeros((\u001b[32m9\u001b[39m,\u001b[32m6\u001b[39m), dtype=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hand \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hands[i].values():\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     cards_remaining += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_hand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (np.sum(cards_remaining, axis=\u001b[32m1\u001b[39m) > \u001b[32m0\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/verify_data.py:173\u001b[39m, in \u001b[36mFishGame.encode_hand\u001b[39m\u001b[34m(self, hand, flatten)\u001b[39m\n\u001b[32m    171\u001b[39m hand_vector = np.zeros((\u001b[32m9\u001b[39m,\u001b[32m6\u001b[39m), dtype=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m card \u001b[38;5;129;01min\u001b[39;00m hand:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     hand_vector[np.where(sets_array == card)[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]][\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43msets_array\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mcard\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]] = \u001b[32m1\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hand_vector.flatten() \u001b[38;5;28;01mif\u001b[39;00m flatten \u001b[38;5;28;01melse\u001b[39;00m hand_vector\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent.train_self_play(500, update_rate=5, epochs=5, path='models/fish_agent_20250406_213513.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
