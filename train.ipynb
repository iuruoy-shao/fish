{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import QLearningAgent\n",
    "from verify_data import FishGame, ParseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/1-15_11:15.txt', 'r') as f:\n",
    "    game = FishGame(f.readlines())\n",
    "    game.rotate('PB')\n",
    "    state = game.to_state()\n",
    "with open('test.txt', 'w') as f:\n",
    "    f.writelines([f'{row.astype(int).tolist()}\\n' for row in state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-3_15:27.txt\n",
      "12-10_11:07.txt\n",
      "12-3_14:05.txt\n",
      "12-3_11:30.txt\n",
      "12-3_14:27.txt\n",
      "12-6_11:08.txt\n",
      "12-3_11:12.txt\n",
      "12-4_11:11.txt\n",
      "1-15_11:15.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# TODO: implement shuffling of teammmates & opponents\n",
    "memories = []\n",
    "for filename in os.listdir('data'):\n",
    "    filepath = os.path.join('data', filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        try:\n",
    "            print(f\"{filename}\")\n",
    "            game = FishGame(f.readlines())\n",
    "            memories.extend(game.memory(player) for player in game.players)\n",
    "        except ParseError as e:\n",
    "            print(f\"{filename}: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/Fish/.env/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/Users/Yourui/Documents/Fish/agent.py:77: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  return torch.BoolTensor(x).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 6.61067, lr 0.001\n",
      "epoch 1, train loss 6.17041, lr 0.001\n",
      "epoch 2, train loss 5.93276, lr 0.001\n",
      "epoch 3, train loss 5.83383, lr 0.001\n"
     ]
    }
   ],
   "source": [
    "agent = QLearningAgent()\n",
    "agent.train_on_data(memories, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'call': tensor([[0.2981, 0.2519]], device='mps:0', grad_fn=<LinearBackward0>),\n",
       " 'call_set': tensor([[0.1280, 0.3185, 0.5119, 0.2538, 0.3017, 0.1455, 0.2247, 0.2644, 0.1221]],\n",
       "        device='mps:0', grad_fn=<MaskedFillBackward0>),\n",
       " 'call_cards': tensor([[[ 2.8740e-01,  1.7798e-01,  3.1982e-01, -1.0000e+09],\n",
       "          [ 2.6663e-01,  3.0490e-01,  1.8063e-01, -1.0000e+09],\n",
       "          [ 2.6848e-01,  2.5655e-01,  3.1924e-01, -1.0000e+09],\n",
       "          [ 2.6059e-01,  3.1243e-01,  1.6983e-01, -1.0000e+09],\n",
       "          [ 2.8911e-01,  1.7327e-01,  3.8114e-01, -1.0000e+09],\n",
       "          [ 2.9736e-01,  2.9311e-01,  2.7129e-01, -1.0000e+09]]],\n",
       "        device='mps:0', grad_fn=<MaskedFillBackward0>),\n",
       " 'ask_person': tensor([[ 2.2886e-01,  2.7052e-01,  1.9633e-01, -1.0000e+09]], device='mps:0',\n",
       "        grad_fn=<MaskedFillBackward0>),\n",
       " 'ask_set': tensor([[-1.0000e+09,  2.1722e-01,  2.8336e-01, -1.0000e+09, -1.0000e+09,\n",
       "           2.6968e-01,  2.2872e-01,  2.4010e-01,  1.8760e-01]], device='mps:0',\n",
       "        grad_fn=<MaskedFillBackward0>),\n",
       " 'ask_card': tensor([[0.1473, 0.1618, 0.2547, 0.2045, 0.1627, 0.3009]], device='mps:0',\n",
       "        grad_fn=<LinearBackward0>)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = agent.unpack_memory([memories[0]])\n",
    "agent.q_network(agent.tensor(batch['state']), agent.action_masks(*batch['mask_dep'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/fish_agent_20250409_163520.pth\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_path = f'models/fish_agent_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pth'\n",
    "agent.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = QLearningAgent() \n",
    "agent.load_model('models/fish_agent_20250409_163520.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0 finished, 2069 memories collected\n",
      "Game 1 finished, 2069 memories collected\n",
      "epoch 0, train loss 15.62397, test loss None, lr 0.001\n",
      "epoch 1, train loss 9.17356, test loss None, lr 0.001\n",
      "epoch 2, train loss 8.2876, test loss None, lr 0.001\n",
      "epoch 3, train loss 2.8894, test loss None, lr 0.001\n",
      "epoch 4, train loss 5.79992, test loss None, lr 0.001\n",
      "Game 2 finished, 2069 memories collected\n",
      "epoch 0, train loss 2.7978, test loss None, lr 0.001\n",
      "epoch 1, train loss 1.8704, test loss None, lr 0.001\n",
      "epoch 2, train loss 0.7184, test loss None, lr 0.001\n",
      "epoch 3, train loss 1.3261, test loss None, lr 0.001\n",
      "epoch 4, train loss 0.8529, test loss None, lr 0.001\n",
      "Game 3 finished, 2069 memories collected\n",
      "epoch 0, train loss 2.10057, test loss None, lr 0.001\n",
      "epoch 1, train loss 0.79886, test loss None, lr 0.001\n",
      "epoch 2, train loss 0.6223, test loss None, lr 0.001\n",
      "epoch 3, train loss 0.29207, test loss None, lr 0.001\n",
      "epoch 4, train loss 0.26838, test loss None, lr 0.001\n",
      "epoch 0, train loss 0.30288, test loss None, lr 0.001\n",
      "epoch 1, train loss 0.50208, test loss None, lr 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_self_play\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/fish_agent_20250407_145324.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/agent.py:230\u001b[39m, in \u001b[36mQLearningAgent.train_self_play\u001b[39m\u001b[34m(self, n_games, update_rate, epochs, path)\u001b[39m\n\u001b[32m    228\u001b[39m     memories_batch = []\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % (update_rate * \u001b[32m3\u001b[39m) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(memories):\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28mself\u001b[39m.pickle_memory(memories)\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mself\u001b[39m.save_model(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/agent.py:189\u001b[39m, in \u001b[36mQLearningAgent.train_on_data\u001b[39m\u001b[34m(self, train_memory, test_memory, n_epochs, lr_schedule)\u001b[39m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    188\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.pick_batch(\u001b[38;5;28mself\u001b[39m.memory, (i,i+\u001b[38;5;28mself\u001b[39m.batch_size))\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m total_loss += loss\n\u001b[32m    191\u001b[39m batch_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/agent.py:169\u001b[39m, in \u001b[36mQLearningAgent.handle_batch\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[32m    168\u001b[39m     current_q = \u001b[38;5;28mself\u001b[39m.q_network(\u001b[38;5;28mself\u001b[39m.tensor(batch[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m]), batch[\u001b[33m'\u001b[39m\u001b[33maction_masks\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     next_q = \u001b[38;5;28mself\u001b[39m.q_network(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnext_state\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, batch[\u001b[33m'\u001b[39m\u001b[33mnext_action_masks\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.q_loss(current_q, next_q, batch[\u001b[33m'\u001b[39m\u001b[33maction\u001b[39m\u001b[33m'\u001b[39m], batch[\u001b[33m'\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/agent.py:82\u001b[39m, in \u001b[36mQLearningAgent.tensor\u001b[39m\u001b[34m(self, x, as_bool)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m as_bool:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.BoolTensor(x).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent.train_self_play(2000, update_rate=1, epochs=5, path='models/fish_agent_20250407_145324.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
