{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import QLearningAgent, QNetwork\n",
    "from verify_data import FishGame, ParseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-10_11:07.txt\n",
      "12-3_14:05.txt\n",
      "12-3_11:30.txt\n",
      "12-3_14:27.txt\n",
      "12-6_11:08.txt\n",
      "12-3_11:12.txt\n",
      "1-15_11:15.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "memories = []\n",
    "for filename in os.listdir('data'):\n",
    "    filepath = os.path.join('data', filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        try:\n",
    "            print(f\"{filename}\")\n",
    "            game = FishGame(f.readlines())\n",
    "            for player in game.players:\n",
    "                memories += game.memory(player)\n",
    "        except ParseError as e:\n",
    "            print(f\"{filename}: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/Fish/nn.py:63: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  return torch.FloatTensor(x).to(self.device)\n",
      "/Users/Yourui/Documents/Fish/nn.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  'call_set': F.softmax(call_set.masked_fill(~action_masks['call_set'], -1e9)),\n",
      "/Users/Yourui/Documents/Fish/nn.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  'ask_person': F.softmax(ask_person.masked_fill(~action_masks['ask_person'], -1e9)),\n",
      "/Users/Yourui/Documents/Fish/nn.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  'ask_set': F.softmax(ask_set.masked_fill(~action_masks['ask_set'], -1e9)),\n",
      "/Users/Yourui/Documents/Fish/nn.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  'ask_card': F.softmax(ask_card),\n",
      "/Users/Yourui/Documents/Fish/nn.py:79: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = lambda prev, next, player, reward: F.mse_loss(sum(prev * player),\n",
      "/Users/Yourui/Documents/Fish/nn.py:79: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = lambda prev, next, player, reward: F.mse_loss(sum(prev * player),\n",
      "/Users/Yourui/Documents/Fish/nn.py:79: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = lambda prev, next, player, reward: F.mse_loss(sum(prev * player),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2155.8910027874517, lr 0.001\n",
      "epoch 1, loss 2401.6255965039345, lr 0.001\n",
      "epoch 2, loss 3313.0624049901962, lr 0.001\n",
      "epoch 3, loss 3456.0317876338963, lr 0.001\n",
      "epoch 4, loss 2985.931401371956, lr 0.001\n",
      "epoch 5, loss 3316.2661924462136, lr 0.001\n",
      "epoch 6, loss 3357.921067595482, lr 0.001\n",
      "epoch 7, loss 3569.264308691025, lr 0.001\n",
      "epoch 8, loss 3519.075119495392, lr 0.001\n",
      "epoch 9, loss 3169.063161969185, lr 0.001\n",
      "epoch 10, loss 2986.947627544403, lr 0.001\n",
      "epoch 11, loss 2987.128251552582, lr 0.001\n",
      "epoch 12, loss 3113.6692420244217, lr 0.001\n",
      "epoch 13, loss 3739.700010061264, lr 0.001\n",
      "epoch 14, loss 3830.7274074554443, lr 0.001\n",
      "epoch 15, loss 3830.7274508476257, lr 0.001\n",
      "epoch 16, loss 3537.3545479774475, lr 0.001\n",
      "epoch 17, loss 3376.8869260549545, lr 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m agent = QLearningAgent(memories)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/nn.py:127\u001b[39m, in \u001b[36mQLearningAgent.train\u001b[39m\u001b[34m(self, n_epochs)\u001b[39m\n\u001b[32m    125\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.unpack_batch(\u001b[38;5;28mself\u001b[39m.memory[i:i + \u001b[38;5;28mself\u001b[39m.batch_size])\n\u001b[32m    126\u001b[39m current_q = \u001b[38;5;28mself\u001b[39m.q_network(\u001b[38;5;28mself\u001b[39m.tensor(batch[\u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m]), \u001b[38;5;28mself\u001b[39m.action_masks(*batch[\u001b[33m'\u001b[39m\u001b[33mmask_dep\u001b[39m\u001b[33m'\u001b[39m].values()))\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m next_q = \u001b[38;5;28mself\u001b[39m.q_network(\u001b[38;5;28mself\u001b[39m.tensor(batch[\u001b[33m'\u001b[39m\u001b[33mnext_state\u001b[39m\u001b[33m'\u001b[39m]), \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmask_dep\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    129\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.q_loss(current_q, next_q, batch[\u001b[33m'\u001b[39m\u001b[33maction\u001b[39m\u001b[33m'\u001b[39m], batch[\u001b[33m'\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    130\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/nn.py:98\u001b[39m, in \u001b[36mQLearningAgent.action_masks\u001b[39m\u001b[34m(self, agent_index, hand, sets_remaining, cards_remaining)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maction_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent_index, hand, sets_remaining, cards_remaining):\n\u001b[32m     96\u001b[39m     cards_remaining = np.array(cards_remaining)\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcall_set\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msets_remaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_bool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m, \u001b[38;5;66;03m# the sets that remain\u001b[39;00m\n\u001b[32m     99\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcall_cards\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.tensor([np.tile(np.array(cards_remaining)[i,index%\u001b[32m2\u001b[39m::\u001b[32m2\u001b[39m] > \u001b[32m0\u001b[39m, (\u001b[32m6\u001b[39m,\u001b[32m1\u001b[39m)) \n\u001b[32m    100\u001b[39m                                    \u001b[38;5;28;01mfor\u001b[39;00m i, index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agent_index)], as_bool=\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;66;03m# the players on the team that still have cards\u001b[39;00m\n\u001b[32m    101\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mask_person\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.tensor([np.array(cards_remaining)[i,(index+\u001b[32m1\u001b[39m)%\u001b[32m2\u001b[39m::\u001b[32m2\u001b[39m] > \u001b[32m0\u001b[39m \n\u001b[32m    102\u001b[39m                                    \u001b[38;5;28;01mfor\u001b[39;00m i, index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agent_index)], as_bool=\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;66;03m# the players on the opposing team that still have cards\u001b[39;00m\n\u001b[32m    103\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mask_set\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.tensor(np.sum(hand, axis=\u001b[32m2\u001b[39m) > \u001b[32m0\u001b[39m, as_bool=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# the sets that the player holds\u001b[39;00m\n\u001b[32m    104\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Fish/nn.py:62\u001b[39m, in \u001b[36mQLearningAgent.tensor\u001b[39m\u001b[34m(self, x, as_bool)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, as_bool=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m as_bool:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBoolTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.FloatTensor(x).to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent = QLearningAgent(memories)\n",
    "agent.train(200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
