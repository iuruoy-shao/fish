{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import QLearningAgent\n",
    "from verify_data import FishGame, ParseError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "\n",
    "def process_file(filename, data_dir='data'):\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    file_memories = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            print(f\"{filename}\")\n",
    "            game = FishGame(f.readlines())\n",
    "            for player in game.players:\n",
    "                for _ in range(100):\n",
    "                    game.shuffle()\n",
    "                    file_memories.append(game.memory(player))\n",
    "        return file_memories\n",
    "    except ParseError as e:\n",
    "        print(f\"{filename}: {e}\")\n",
    "        return []\n",
    "\n",
    "if os.path.isfile('memories_extended.pkl'):\n",
    "    with open('memories_extended.pkl', 'rb') as f:\n",
    "        memories = pickle.load(f)\n",
    "else:\n",
    "    memories = []\n",
    "    filenames = os.listdir('data')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        num_processes = min(mp.cpu_count(), 8)\n",
    "        with mp.Pool(processes=num_processes) as pool:\n",
    "            results = pool.map(process_file, filenames)\n",
    "        for result in results:\n",
    "            memories.extend(result)\n",
    "    else:\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join('data', filename)\n",
    "            with open(filepath, 'r') as f:\n",
    "                try:\n",
    "                    print(f\"{filename}\")\n",
    "                    game = FishGame(f.readlines())\n",
    "                    for player in game.players:\n",
    "                        for _ in range(100):\n",
    "                            game.shuffle()\n",
    "                            memories.append(game.memory(player))\n",
    "                except ParseError as e:\n",
    "                    print(f\"{filename}: {e}\")\n",
    "                    break\n",
    "    with open('memories_extended.pkl', 'wb') as f:\n",
    "        pickle.dump(memories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('call_memories.pkl', 'rb') as f:\n",
    "    memories = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading hand predictor\n",
      "loading q vals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Q-Network epoch 213 train loss 0.69419 test loss 0.62034 lr 1e-06:  71%|███████▏  | 214/300 [01:45<00:42,  2.03it/s]                 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m agent = QLearningAgent()\n\u001b[32m      2\u001b[39m agent.load_model(\u001b[33m'\u001b[39m\u001b[33mmodels/fish_agent.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Fish/agent.py:315\u001b[39m, in \u001b[36mQLearningAgent.train_on_data\u001b[39m\u001b[34m(self, memory, q_epochs, hand_epochs, lr_schedule, use_tqdm)\u001b[39m\n\u001b[32m    313\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_hand_predictor(hand_epochs, lr_schedule)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m q_epochs:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_q_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Fish/agent.py:254\u001b[39m, in \u001b[36mQLearningAgent.train_q_network\u001b[39m\u001b[34m(self, n_epochs, lr_schedule, use_tqdm)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28mself\u001b[39m.q_network.eval()\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     test_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_q_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_hands\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lr_schedule:\n\u001b[32m    257\u001b[39m     \u001b[38;5;28mself\u001b[39m.q_scheduler.step(test_loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Fish/agent.py:213\u001b[39m, in \u001b[36mQLearningAgent.handle_q_batch\u001b[39m\u001b[34m(self, batch, pred_hands)\u001b[39m\n\u001b[32m    211\u001b[39m current_q = \u001b[38;5;28mself\u001b[39m.q_network(\u001b[38;5;28mself\u001b[39m.tensor(batch[\u001b[33m'\u001b[39m\u001b[33mpredicted_hands\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred_hands \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mhands\u001b[39m\u001b[33m'\u001b[39m]), batch[\u001b[33m'\u001b[39m\u001b[33maction_masks\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    212\u001b[39m next_q = \u001b[38;5;28mself\u001b[39m.q_network(\u001b[38;5;28mself\u001b[39m.tensor(batch[\u001b[33m'\u001b[39m\u001b[33mnext_predicted_hands\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred_hands \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mnext_hands\u001b[39m\u001b[33m'\u001b[39m]), batch[\u001b[33m'\u001b[39m\u001b[33mnext_action_masks\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maction\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreward\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Fish/agent.py:139\u001b[39m, in \u001b[36mQLearningAgent.q_loss\u001b[39m\u001b[34m(self, action, next_action, player_action, reward)\u001b[39m\n\u001b[32m    137\u001b[39m             rewards.append(this_reward)\n\u001b[32m    138\u001b[39m             next_qs.append(this_next_q)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(\u001b[38;5;28mself\u001b[39m.current_q(pad_sequence(agent_actions), \u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer_actions\u001b[49m\u001b[43m)\u001b[49m), \n\u001b[32m    140\u001b[39m                  \u001b[38;5;28mself\u001b[39m.target_q(torch.stack(next_qs), \u001b[38;5;28mself\u001b[39m.tensor(rewards)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Fish/.venv/lib/python3.13/site-packages/torch/nn/utils/rnn.py:481\u001b[39m, in \u001b[36mpad_sequence\u001b[39m\u001b[34m(sequences, batch_first, padding_value, padding_side)\u001b[39m\n\u001b[32m    477\u001b[39m         sequences = sequences.unbind(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    483\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent = QLearningAgent()\n",
    "agent.load_model('models/fish_agent.pth')\n",
    "agent.train_on_data(memories, 300, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = agent.unpack_memory([memories[3][15]])\n",
    "agent.hand_predictor(agent.tensor(batch['state']), agent.action_masks(*batch['mask_dep'].values())['hands'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/fish_agent.pth\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "# model_path = f'models/fish_agent_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pth'\n",
    "model_path = 'models/fish_agent.pth'\n",
    "agent.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import QLearningAgent\n",
    "agent = QLearningAgent(memories) \n",
    "agent.load_model('models/fish_agent.pth')\n",
    "agent.train_self_play(2000, update_rate=1, hand_epochs=10, q_epochs=5, path='models/fish_agent.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
